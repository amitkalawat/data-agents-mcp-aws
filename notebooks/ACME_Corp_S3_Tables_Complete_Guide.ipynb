{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACME Corp S3 Tables Complete Guide\n",
    "\n",
    "This notebook provides a comprehensive guide for working with ACME Corp data in S3 Tables format, integrated with SageMaker Lakehouse and queryable via Amazon Athena.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#1-environment-setup)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "3. [S3 Upload and Configuration](#3-s3-upload)\n",
    "4. [SageMaker Lakehouse Setup](#4-sagemaker-lakehouse)\n",
    "5. [Athena Query Examples](#5-athena-queries)\n",
    "6. [MCP Server Integration](#6-mcp-server)\n",
    "7. [Troubleshooting](#7-troubleshooting)\n",
    "8. [Advanced Analytics](#8-advanced-analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a id='1-environment-setup'></a>\n",
    "\n",
    "First, let's install and import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas pyarrow boto3 matplotlib seaborn plotly --quiet\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Configuration\n",
    "\n",
    "Configure your AWS credentials and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_REGION = os.getenv('AWS_REGION', 'us-west-2')\n",
    "AWS_PROFILE = os.getenv('AWS_PROFILE', 'default')\n",
    "\n",
    "# Initialize AWS clients\n",
    "session = boto3.Session(region_name=AWS_REGION)\n",
    "s3 = session.client('s3')\n",
    "glue = session.client('glue')\n",
    "athena = session.client('athena')\n",
    "sts = session.client('sts')\n",
    "\n",
    "# Get account ID\n",
    "ACCOUNT_ID = sts.get_caller_identity()['Account']\n",
    "\n",
    "# Define bucket names\n",
    "S3_BUCKET = f'acme-corp-lakehouse-{ACCOUNT_ID}'\n",
    "ATHENA_RESULTS_BUCKET = f'{S3_BUCKET}/athena-results/'\n",
    "\n",
    "print(f\"üîß AWS Configuration:\")\n",
    "print(f\"   Region: {AWS_REGION}\")\n",
    "print(f\"   Account ID: {ACCOUNT_ID}\")\n",
    "print(f\"   S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation <a id='2-data-preparation'></a>\n",
    "\n",
    "Convert ACME Corp CSV data to Parquet format for S3 Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_s3_tables():\n",
    "    \"\"\"\n",
    "    Convert CSV files to Parquet format for S3 Tables\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    s3_tables_dir = \"s3_tables_format\"\n",
    "    os.makedirs(s3_tables_dir, exist_ok=True)\n",
    "    \n",
    "    # Define data directories\n",
    "    data_dirs = {\n",
    "        \"ad_campaign\": \"ad_campaign_data\",\n",
    "        \"streaming\": \"streaming_analytics\", \n",
    "        \"users\": \"user_details\"\n",
    "    }\n",
    "    \n",
    "    converted_files = []\n",
    "    table_stats = []\n",
    "    \n",
    "    for category, dir_name in data_dirs.items():\n",
    "        category_dir = os.path.join(s3_tables_dir, category)\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all CSV files\n",
    "        csv_files = list(Path(dir_name).glob(\"*.csv\"))\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                # Read CSV\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Convert to Parquet\n",
    "                table_name = Path(csv_file).stem\n",
    "                output_path = os.path.join(category_dir, f\"{table_name}.parquet\")\n",
    "                df.to_parquet(output_path, engine='pyarrow', compression='snappy')\n",
    "                \n",
    "                # Collect statistics\n",
    "                stats = {\n",
    "                    'table': table_name,\n",
    "                    'category': category,\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'size_mb': os.path.getsize(output_path) / (1024 * 1024)\n",
    "                }\n",
    "                table_stats.append(stats)\n",
    "                \n",
    "                print(f\"‚úÖ Converted {table_name}: {len(df):,} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error converting {csv_file}: {e}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    stats_df = pd.DataFrame(table_stats)\n",
    "    display(stats_df)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Total tables: {len(stats_df)}\")\n",
    "    print(f\"   Total rows: {stats_df['rows'].sum():,}\")\n",
    "    print(f\"   Total size: {stats_df['size_mb'].sum():.2f} MB\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Run data preparation\n",
    "if os.path.exists('ad_campaign_data'):\n",
    "    table_stats = prepare_s3_tables()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Data directories not found. Please ensure ACME Corp data is in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Check\n",
    "\n",
    "Let's verify the data quality and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(parquet_file):\n",
    "    \"\"\"\n",
    "    Perform data quality checks on a Parquet file\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    print(f\"\\nüìã Data Quality Report: {os.path.basename(parquet_file)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nShape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"\\n‚ö†Ô∏è  Missing values:\")\n",
    "        display(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìä Data types:\")\n",
    "    display(df.dtypes.value_counts())\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\nüîç Sample data (first 5 rows):\")\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Check a sample file\n",
    "sample_file = \"s3_tables_format/users/user_details.parquet\"\n",
    "if os.path.exists(sample_file):\n",
    "    sample_df = check_data_quality(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. S3 Upload and Configuration <a id='3-s3-upload'></a>\n",
    "\n",
    "Upload the prepared Parquet files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name, region='us-west-2'):\n",
    "    \"\"\"\n",
    "    Create S3 bucket if it doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "        print(f\"‚úÖ Created bucket: {bucket_name}\")\n",
    "    except s3.exceptions.BucketAlreadyExists:\n",
    "        print(f\"‚ÑπÔ∏è  Bucket already exists: {bucket_name}\")\n",
    "    except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "        print(f\"‚ÑπÔ∏è  You already own bucket: {bucket_name}\")\n",
    "\n",
    "def upload_to_s3(local_path, s3_path):\n",
    "    \"\"\"\n",
    "    Upload file to S3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3.upload_file(local_path, S3_BUCKET, s3_path)\n",
    "        print(f\"‚úÖ Uploaded: {s3_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading {local_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_all_tables():\n",
    "    \"\"\"\n",
    "    Upload all Parquet files to S3\n",
    "    \"\"\"\n",
    "    # Create bucket\n",
    "    create_s3_bucket(S3_BUCKET, AWS_REGION)\n",
    "    \n",
    "    # Upload files\n",
    "    upload_count = 0\n",
    "    s3_tables_dir = \"s3_tables_format\"\n",
    "    \n",
    "    for category in ['users', 'streaming', 'ad_campaign']:\n",
    "        category_dir = os.path.join(s3_tables_dir, category)\n",
    "        if os.path.exists(category_dir):\n",
    "            for parquet_file in Path(category_dir).glob(\"*.parquet\"):\n",
    "                table_name = parquet_file.stem\n",
    "                s3_key = f\"tables/{category}/{table_name}/{table_name}.parquet\"\n",
    "                \n",
    "                if upload_to_s3(str(parquet_file), s3_key):\n",
    "                    upload_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Upload complete! {upload_count} files uploaded to S3.\")\n",
    "    return upload_count\n",
    "\n",
    "# Upload tables to S3\n",
    "# upload_count = upload_all_tables()\n",
    "print(\"‚ö†Ô∏è  Uncomment the line above to actually upload files to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SageMaker Lakehouse Setup <a id='4-sagemaker-lakehouse'></a>\n",
    "\n",
    "Set up the Glue Data Catalog for SageMaker Lakehouse integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glue database configuration\n",
    "DATABASE_NAME = 'acme_corp_lakehouse'\n",
    "\n",
    "def create_glue_database():\n",
    "    \"\"\"\n",
    "    Create Glue database for SageMaker Lakehouse\n",
    "    \"\"\"\n",
    "    try:\n",
    "        glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': DATABASE_NAME,\n",
    "                'Description': 'ACME Corp SageMaker Lakehouse database',\n",
    "                'LocationUri': f's3://{S3_BUCKET}/databases/{DATABASE_NAME}/'\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ Created Glue database: {DATABASE_NAME}\")\n",
    "    except glue.exceptions.AlreadyExistsException:\n",
    "        print(f\"‚ÑπÔ∏è  Database already exists: {DATABASE_NAME}\")\n",
    "\n",
    "def get_table_schema(table_name):\n",
    "    \"\"\"\n",
    "    Define table schemas for Glue\n",
    "    \"\"\"\n",
    "    schemas = {\n",
    "        \"user_details\": [\n",
    "            {\"Name\": \"user_id\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"email\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"age\", \"Type\": \"bigint\"},\n",
    "            {\"Name\": \"gender\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"country\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"city\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"subscription_plan\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"monthly_price\", \"Type\": \"double\"},\n",
    "            {\"Name\": \"signup_date\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"is_active\", \"Type\": \"boolean\"},\n",
    "            {\"Name\": \"last_payment_date\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"primary_device\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"num_profiles\", \"Type\": \"bigint\"},\n",
    "            {\"Name\": \"payment_method\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"lifetime_value\", \"Type\": \"double\"},\n",
    "            {\"Name\": \"referral_source\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"language_preference\", \"Type\": \"string\"}\n",
    "        ],\n",
    "        \"campaigns\": [\n",
    "            {\"Name\": \"campaign_id\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"campaign_name\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"campaign_type\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"objective\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"start_date\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"end_date\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"budget\", \"Type\": \"double\"},\n",
    "            {\"Name\": \"target_audience\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"target_countries\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"promoted_content_id\", \"Type\": \"string\"},\n",
    "            {\"Name\": \"promoted_content_title\", \"Type\": \"string\"}\n",
    "        ]\n",
    "        # Add other table schemas as needed\n",
    "    }\n",
    "    return schemas.get(table_name, [])\n",
    "\n",
    "def register_glue_table(table_name, s3_location, schema):\n",
    "    \"\"\"\n",
    "    Register table in Glue Data Catalog\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Delete existing table if any\n",
    "        try:\n",
    "            glue.delete_table(DatabaseName=DATABASE_NAME, Name=table_name)\n",
    "            print(f\"üóëÔ∏è  Deleted existing table: {table_name}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create table with correct Parquet configuration\n",
    "        table_input = {\n",
    "            'Name': table_name,\n",
    "            'StorageDescriptor': {\n",
    "                'Columns': schema,\n",
    "                'Location': s3_location,\n",
    "                'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
    "                'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
    "                'SerdeInfo': {\n",
    "                    'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
    "                    'Parameters': {'serialization.format': '1'}\n",
    "                },\n",
    "                'StoredAsSubDirectories': False\n",
    "            },\n",
    "            'PartitionKeys': [],\n",
    "            'TableType': 'EXTERNAL_TABLE',\n",
    "            'Parameters': {\n",
    "                'EXTERNAL': 'TRUE',\n",
    "                'parquet.compression': 'SNAPPY',\n",
    "                'classification': 'parquet'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        glue.create_table(\n",
    "            DatabaseName=DATABASE_NAME,\n",
    "            TableInput=table_input\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Registered table: {table_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error registering table {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Register a table\n",
    "# create_glue_database()\n",
    "# schema = get_table_schema('user_details')\n",
    "# register_glue_table('user_details', f's3://{S3_BUCKET}/tables/users/user_details/', schema)\n",
    "print(\"‚ö†Ô∏è  Uncomment the lines above to create Glue database and tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Athena Query Examples <a id='5-athena-queries'></a>\n",
    "\n",
    "Execute queries using Amazon Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AthenaQueryExecutor:\n",
    "    \"\"\"\n",
    "    Helper class for executing Athena queries\n",
    "    \"\"\"\n",
    "    def __init__(self, database=DATABASE_NAME, workgroup='primary'):\n",
    "        self.athena = athena\n",
    "        self.database = database\n",
    "        self.workgroup = workgroup\n",
    "        self.output_location = f's3://{S3_BUCKET}/athena-results/'\n",
    "    \n",
    "    def execute_query(self, query, max_wait=30):\n",
    "        \"\"\"\n",
    "        Execute Athena query and return results as DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Start query execution\n",
    "            response = self.athena.start_query_execution(\n",
    "                QueryString=query,\n",
    "                QueryExecutionContext={'Database': self.database},\n",
    "                ResultConfiguration={'OutputLocation': self.output_location}\n",
    "            )\n",
    "            \n",
    "            query_id = response['QueryExecutionId']\n",
    "            print(f\"üîÑ Query ID: {query_id}\")\n",
    "            \n",
    "            # Wait for completion\n",
    "            for i in range(max_wait):\n",
    "                result = self.athena.get_query_execution(QueryExecutionId=query_id)\n",
    "                status = result['QueryExecution']['Status']['State']\n",
    "                \n",
    "                if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "                    break\n",
    "                    \n",
    "                time.sleep(1)\n",
    "                print(f\"‚è≥ Waiting... ({i+1}s)\", end='\\r')\n",
    "            \n",
    "            if status == 'SUCCEEDED':\n",
    "                print(f\"\\n‚úÖ Query succeeded!\")\n",
    "                \n",
    "                # Get results\n",
    "                results = self.athena.get_query_results(QueryExecutionId=query_id)\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                rows = results['ResultSet']['Rows']\n",
    "                if len(rows) > 1:\n",
    "                    columns = [col['VarCharValue'] for col in rows[0]['Data']]\n",
    "                    data = [[col.get('VarCharValue', None) for col in row['Data']] \n",
    "                            for row in rows[1:]]\n",
    "                    return pd.DataFrame(data, columns=columns)\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            else:\n",
    "                error = result['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')\n",
    "                print(f\"\\n‚ùå Query failed: {error}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize query executor\n",
    "query_executor = AthenaQueryExecutor()\n",
    "\n",
    "# Example queries\n",
    "example_queries = {\n",
    "    \"user_stats\": \"\"\"\n",
    "        SELECT \n",
    "            subscription_plan,\n",
    "            COUNT(*) as user_count,\n",
    "            AVG(CAST(monthly_price AS DOUBLE)) as avg_price,\n",
    "            SUM(CASE WHEN is_active = true THEN 1 ELSE 0 END) as active_users\n",
    "        FROM user_details\n",
    "        GROUP BY subscription_plan\n",
    "        ORDER BY user_count DESC\n",
    "    \"\"\",\n",
    "    \n",
    "    \"payment_analysis\": \"\"\"\n",
    "        SELECT \n",
    "            payment_method,\n",
    "            COUNT(*) as users,\n",
    "            ROUND(AVG(lifetime_value), 2) as avg_ltv,\n",
    "            ROUND(SUM(lifetime_value), 2) as total_ltv\n",
    "        FROM user_details\n",
    "        GROUP BY payment_method\n",
    "        ORDER BY total_ltv DESC\n",
    "    \"\"\",\n",
    "    \n",
    "    \"device_distribution\": \"\"\"\n",
    "        SELECT \n",
    "            primary_device,\n",
    "            subscription_plan,\n",
    "            COUNT(*) as user_count\n",
    "        FROM user_details\n",
    "        WHERE is_active = true\n",
    "        GROUP BY primary_device, subscription_plan\n",
    "        ORDER BY primary_device, user_count DESC\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Execute a sample query\n",
    "# result_df = query_executor.execute_query(example_queries['user_stats'])\n",
    "# if result_df is not None:\n",
    "#     display(result_df)\n",
    "print(\"‚ö†Ô∏è  Uncomment the lines above to execute Athena queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Visualization\n",
    "\n",
    "Visualize query results with interactive charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subscription_distribution(df):\n",
    "    \"\"\"\n",
    "    Create interactive visualization of subscription distribution\n",
    "    \"\"\"\n",
    "    # Convert string numbers to numeric\n",
    "    df['user_count'] = pd.to_numeric(df['user_count'])\n",
    "    df['active_users'] = pd.to_numeric(df['active_users'])\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bars for total users\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Total Users',\n",
    "        x=df['subscription_plan'],\n",
    "        y=df['user_count'],\n",
    "        text=df['user_count'],\n",
    "        textposition='auto',\n",
    "    ))\n",
    "    \n",
    "    # Add bars for active users\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Active Users',\n",
    "        x=df['subscription_plan'],\n",
    "        y=df['active_users'],\n",
    "        text=df['active_users'],\n",
    "        textposition='auto',\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Subscription Plan Distribution',\n",
    "        xaxis_title='Subscription Plan',\n",
    "        yaxis_title='Number of Users',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example: Create visualization from sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'subscription_plan': ['Premium', 'Standard', 'Basic', 'Premium Plus'],\n",
    "    'user_count': [3143, 2992, 2238, 1627],\n",
    "    'active_users': [2984, 2702, 1887, 1556]\n",
    "})\n",
    "\n",
    "fig = visualize_subscription_distribution(sample_data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MCP Server Integration <a id='6-mcp-server'></a>\n",
    "\n",
    "Set up and test the AWS Data Processing MCP Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Server Configuration\n",
    "mcp_config = {\n",
    "    \"mcpServers\": {\n",
    "        \"aws-dataprocessing\": {\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"awslabs.aws-dataprocessing-mcp-server@latest\",\n",
    "                \"--allow-write\"\n",
    "            ],\n",
    "            \"env\": {\n",
    "                \"AWS_REGION\": AWS_REGION,\n",
    "                \"AWS_PROFILE\": \"default\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"capabilities\": {\n",
    "        \"athena\": {\n",
    "            \"enabled\": True,\n",
    "            \"workgroup\": \"primary\",\n",
    "            \"output_location\": f\"s3://{S3_BUCKET}/athena-query-results/\",\n",
    "            \"database\": DATABASE_NAME\n",
    "        },\n",
    "        \"glue\": {\n",
    "            \"enabled\": True,\n",
    "            \"catalog_id\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã MCP Server Configuration:\")\n",
    "print(json.dumps(mcp_config, indent=2))\n",
    "\n",
    "# Installation instructions\n",
    "print(\"\\nüì¶ To install MCP Server:\")\n",
    "print(\"1. Install uv: curl -LsSf https://astral.sh/uv/install.sh | sh\")\n",
    "print(\"2. Install server: uvx awslabs.aws-dataprocessing-mcp-server@latest\")\n",
    "print(\"3. Save configuration to ~/.config/mcp/aws-dataprocessing.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Agent Query Examples\n",
    "\n",
    "Examples of how AI agents can query the data using natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language query examples\n",
    "nl_queries = [\n",
    "    {\n",
    "        \"question\": \"What is the average lifetime value of Premium subscribers?\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT \n",
    "                subscription_plan,\n",
    "                COUNT(*) as user_count,\n",
    "                ROUND(AVG(lifetime_value), 2) as avg_lifetime_value,\n",
    "                ROUND(MIN(lifetime_value), 2) as min_ltv,\n",
    "                ROUND(MAX(lifetime_value), 2) as max_ltv\n",
    "            FROM user_details\n",
    "            WHERE subscription_plan = 'Premium'\n",
    "            GROUP BY subscription_plan\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which payment methods are most popular among active users?\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT \n",
    "                payment_method,\n",
    "                COUNT(*) as active_user_count,\n",
    "                ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "            FROM user_details\n",
    "            WHERE is_active = true\n",
    "            GROUP BY payment_method\n",
    "            ORDER BY active_user_count DESC\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Show me the geographic distribution of Premium Plus subscribers\",\n",
    "        \"sql\": \"\"\"\n",
    "            SELECT \n",
    "                country,\n",
    "                COUNT(*) as subscriber_count,\n",
    "                COUNT(DISTINCT city) as cities_count\n",
    "            FROM user_details\n",
    "            WHERE subscription_plan = 'Premium Plus'\n",
    "            GROUP BY country\n",
    "            ORDER BY subscriber_count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display natural language queries\n",
    "for i, query in enumerate(nl_queries, 1):\n",
    "    print(f\"\\nü§ñ Query {i}: {query['question']}\")\n",
    "    print(f\"\\nSQL Translation:\")\n",
    "    print(query['sql'])\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting <a id='7-troubleshooting'></a>\n",
    "\n",
    "Common issues and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_athena_issues():\n",
    "    \"\"\"\n",
    "    Diagnose common Athena query issues\n",
    "    \"\"\"\n",
    "    print(\"üîç Athena Diagnostics\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check database exists\n",
    "    try:\n",
    "        response = glue.get_database(Name=DATABASE_NAME)\n",
    "        print(f\"‚úÖ Database '{DATABASE_NAME}' exists\")\n",
    "    except:\n",
    "        print(f\"‚ùå Database '{DATABASE_NAME}' not found\")\n",
    "        return\n",
    "    \n",
    "    # Check tables\n",
    "    try:\n",
    "        response = glue.get_tables(DatabaseName=DATABASE_NAME)\n",
    "        tables = response['TableList']\n",
    "        print(f\"\\nüìã Found {len(tables)} tables:\")\n",
    "        \n",
    "        for table in tables:\n",
    "            table_name = table['Name']\n",
    "            location = table['StorageDescriptor']['Location']\n",
    "            serde = table['StorageDescriptor']['SerdeInfo']['SerializationLibrary']\n",
    "            \n",
    "            print(f\"\\n  Table: {table_name}\")\n",
    "            print(f\"  Location: {location}\")\n",
    "            print(f\"  SerDe: {serde}\")\n",
    "            \n",
    "            # Check if location exists in S3\n",
    "            bucket = location.split('/')[2]\n",
    "            prefix = '/'.join(location.split('/')[3:])\n",
    "            \n",
    "            try:\n",
    "                response = s3.list_objects_v2(\n",
    "                    Bucket=bucket,\n",
    "                    Prefix=prefix,\n",
    "                    MaxKeys=1\n",
    "                )\n",
    "                if 'Contents' in response:\n",
    "                    print(f\"  ‚úÖ S3 location exists\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå S3 location is empty\")\n",
    "            except:\n",
    "                print(f\"  ‚ùå Cannot access S3 location\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error checking tables: {e}\")\n",
    "\n",
    "# Run diagnostics\n",
    "# diagnose_athena_issues()\n",
    "print(\"‚ö†Ô∏è  Uncomment the line above to run Athena diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display common issues and solutions\n",
    "issues = [\n",
    "    {\n",
    "        \"issue\": \"HIVE_UNSUPPORTED_FORMAT error\",\n",
    "        \"cause\": \"Incorrect SerDe configuration for Parquet files\",\n",
    "        \"solution\": \"Use ParquetHiveSerDe instead of LazySimpleSerDe\"\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"COLUMN_NOT_FOUND error\",\n",
    "        \"cause\": \"Column name mismatch or wrong data type\",\n",
    "        \"solution\": \"Check column names and types in Glue table definition\"\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"S3 access denied\",\n",
    "        \"cause\": \"Missing IAM permissions\",\n",
    "        \"solution\": \"Add s3:GetObject and s3:ListBucket permissions\"\n",
    "    },\n",
    "    {\n",
    "        \"issue\": \"Query timeout\",\n",
    "        \"cause\": \"Large data scan or complex query\",\n",
    "        \"solution\": \"Use partitions, add filters, or optimize query\"\n",
    "    }\n",
    "]\n",
    "\n",
    "issues_df = pd.DataFrame(issues)\n",
    "display(HTML(issues_df.to_html(index=False, escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analytics <a id='8-advanced-analytics'></a>\n",
    "\n",
    "Complex analytical queries and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced query examples\n",
    "advanced_queries = {\n",
    "    \"cohort_analysis\": \"\"\"\n",
    "        -- Monthly cohort retention analysis\n",
    "        WITH cohorts AS (\n",
    "            SELECT \n",
    "                user_id,\n",
    "                DATE_TRUNC('month', CAST(signup_date AS DATE)) as cohort_month,\n",
    "                subscription_plan,\n",
    "                is_active\n",
    "            FROM user_details\n",
    "        )\n",
    "        SELECT \n",
    "            cohort_month,\n",
    "            subscription_plan,\n",
    "            COUNT(DISTINCT user_id) as cohort_size,\n",
    "            SUM(CASE WHEN is_active = true THEN 1 ELSE 0 END) as active_users,\n",
    "            ROUND(100.0 * SUM(CASE WHEN is_active = true THEN 1 ELSE 0 END) / COUNT(*), 2) as retention_rate\n",
    "        FROM cohorts\n",
    "        GROUP BY cohort_month, subscription_plan\n",
    "        ORDER BY cohort_month DESC, subscription_plan\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ltv_prediction\": \"\"\"\n",
    "        -- Customer lifetime value by segment\n",
    "        SELECT \n",
    "            subscription_plan,\n",
    "            payment_method,\n",
    "            CASE \n",
    "                WHEN age < 25 THEN '18-24'\n",
    "                WHEN age < 35 THEN '25-34'\n",
    "                WHEN age < 45 THEN '35-44'\n",
    "                WHEN age < 55 THEN '45-54'\n",
    "                ELSE '55+'\n",
    "            END as age_group,\n",
    "            COUNT(*) as user_count,\n",
    "            ROUND(AVG(lifetime_value), 2) as avg_ltv,\n",
    "            ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY lifetime_value), 2) as median_ltv,\n",
    "            ROUND(STDDEV(lifetime_value), 2) as ltv_stddev\n",
    "        FROM user_details\n",
    "        WHERE is_active = true\n",
    "        GROUP BY subscription_plan, payment_method, age_group\n",
    "        HAVING COUNT(*) >= 10\n",
    "        ORDER BY avg_ltv DESC\n",
    "    \"\"\",\n",
    "    \n",
    "    \"churn_analysis\": \"\"\"\n",
    "        -- Churn analysis by multiple factors\n",
    "        SELECT \n",
    "            subscription_plan,\n",
    "            primary_device,\n",
    "            COUNT(*) as total_users,\n",
    "            SUM(CASE WHEN is_active = false THEN 1 ELSE 0 END) as churned_users,\n",
    "            ROUND(100.0 * SUM(CASE WHEN is_active = false THEN 1 ELSE 0 END) / COUNT(*), 2) as churn_rate,\n",
    "            ROUND(AVG(CASE WHEN is_active = false THEN lifetime_value ELSE NULL END), 2) as avg_churned_ltv\n",
    "        FROM user_details\n",
    "        GROUP BY subscription_plan, primary_device\n",
    "        HAVING COUNT(*) >= 50\n",
    "        ORDER BY churn_rate DESC\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"üìä Advanced Analytics Queries Available:\")\n",
    "for name, query in advanced_queries.items():\n",
    "    print(f\"\\n‚Ä¢ {name.replace('_', ' ').title()}\")\n",
    "    print(f\"  Query length: {len(query)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Dashboard Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analytics_dashboard():\n",
    "    \"\"\"\n",
    "    Create interactive dashboard components\n",
    "    \"\"\"\n",
    "    # Sample data for visualization\n",
    "    metrics = {\n",
    "        'Total Users': 10000,\n",
    "        'Active Users': 9129,\n",
    "        'Avg Monthly Revenue': 450000,\n",
    "        'Churn Rate': '8.71%',\n",
    "        'Avg LTV': '$438.50'\n",
    "    }\n",
    "    \n",
    "    # Create metrics cards\n",
    "    html = '<div style=\"display: flex; gap: 20px; flex-wrap: wrap;\">'\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        html += f'''\n",
    "        <div style=\"\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            min-width: 150px;\n",
    "            text-align: center;\n",
    "            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "        \">\n",
    "            <h3 style=\"margin: 0; font-size: 14px; opacity: 0.9;\">{metric}</h3>\n",
    "            <p style=\"margin: 10px 0 0 0; font-size: 24px; font-weight: bold;\">{value:,}</p>\n",
    "        </div>\n",
    "        '''\n",
    "    \n",
    "    html += '</div>'\n",
    "    \n",
    "    display(HTML(html))\n",
    "    \n",
    "    # Create sample time series data\n",
    "    dates = pd.date_range('2024-01-01', periods=30, freq='D')\n",
    "    revenue_data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'revenue': np.random.normal(15000, 2000, 30).cumsum(),\n",
    "        'new_users': np.random.poisson(50, 30),\n",
    "        'churned_users': np.random.poisson(5, 30)\n",
    "    })\n",
    "    \n",
    "    # Create time series plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=revenue_data['date'],\n",
    "        y=revenue_data['revenue'],\n",
    "        mode='lines+markers',\n",
    "        name='Cumulative Revenue',\n",
    "        line=dict(color='#667eea', width=3)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Revenue Trend (Sample Data)',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Revenue ($)',\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create dashboard\n",
    "create_analytics_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Covered\n",
    "1. ‚úÖ Data preparation - Converting CSV to Parquet\n",
    "2. ‚úÖ S3 upload - Storing data in S3 Tables format\n",
    "3. ‚úÖ Glue catalog - Registering tables for querying\n",
    "4. ‚úÖ Athena queries - Executing SQL queries on the data\n",
    "5. ‚úÖ MCP server - Enabling AI agent integration\n",
    "6. ‚úÖ Visualizations - Creating interactive charts\n",
    "\n",
    "### Next Steps\n",
    "1. **Production Deployment**\n",
    "   - Set up automated data pipelines\n",
    "   - Implement data quality checks\n",
    "   - Configure monitoring and alerts\n",
    "\n",
    "2. **Advanced Analytics**\n",
    "   - Build ML models using SageMaker\n",
    "   - Create real-time dashboards\n",
    "   - Implement predictive analytics\n",
    "\n",
    "3. **Integration**\n",
    "   - Connect to BI tools (QuickSight, Tableau)\n",
    "   - Set up API endpoints\n",
    "   - Enable cross-account access\n",
    "\n",
    "### Resources\n",
    "- [AWS S3 Tables Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html)\n",
    "- [Amazon Athena Best Practices](https://docs.aws.amazon.com/athena/latest/ug/best-practices.html)\n",
    "- [SageMaker Lakehouse Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/lakehouse.html)\n",
    "- [MCP Server Documentation](https://awslabs.github.io/mcp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}