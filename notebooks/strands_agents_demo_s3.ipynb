{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ACME Corp Data Agents Demo - AWS Lakehouse Edition\n\nThis notebook demonstrates how to query the ACME Corp AWS data lakehouse using:\n- **Amazon Bedrock** with Claude 3.5 Sonnet for natural language processing\n- **AWS Data Processing MCP Server** for standardized data access\n- **Amazon Athena** for SQL query execution\n- **AWS Glue** for data catalog management\n\n## Architecture Overview\n\n```\nUser Query ‚Üí Bedrock (Claude 3.5) ‚Üí SQL ‚Üí Athena ‚Üí S3 Data ‚Üí Results ‚Üí AI Insights\n```\n\n## Prerequisites\n\n1. AWS credentials configured with appropriate permissions\n2. ACME Corp lakehouse already set up (run setup_s3_tables_lakehouse.py)\n3. Python packages: `boto3`, `pandas`, `matplotlib`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport boto3\nimport json\nimport asyncio\nfrom datetime import datetime, timedelta\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n# AWS Bedrock configuration\nbedrock_runtime = boto3.client('bedrock-runtime', region_name='us-west-2')\nMODEL_ID = 'anthropic.claude-3-5-sonnet-20241022-v2:0'\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize MCP Clients\n",
    "\n",
    "First, we'll create MCP clients for each of our data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize AWS clients\nathena = boto3.client('athena', region_name='us-west-2')\nglue = boto3.client('glue', region_name='us-west-2')\n\n# Configuration\nDATABASE_NAME = 'acme_corp_lakehouse'\nOUTPUT_LOCATION = 's3://acme-corp-lakehouse-878687028155/athena-results/'\n\nprint(\"‚úÖ AWS clients initialized successfully!\")\nprint(f\"Database: {DATABASE_NAME}\")\nprint(f\"Model: Claude 3.5 Sonnet via Amazon Bedrock\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Core Functions for Bedrock + Athena Integration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create AI Agent Classes using Bedrock\n\nclass BedrockAgent:\n    \"\"\"Base class for Bedrock-powered agents\"\"\"\n    \n    def __init__(self, name, role, system_prompt):\n        self.name = name\n        self.role = role\n        self.system_prompt = system_prompt\n        self.schemas = get_table_schemas()\n    \n    async def query(self, question):\n        \"\"\"Process a natural language query\"\"\"\n        print(f\"ü§ñ {self.name} processing query...\")\n        \n        # Add role context to the question\n        contextualized_question = f\"{self.system_prompt}\\n\\nQuestion: {question}\"\n        \n        # Generate SQL\n        sql = query_bedrock_for_sql(contextualized_question, self.schemas)\n        print(f\"üìù Generated SQL: {sql}\")\n        \n        # Execute query\n        try:\n            df_result = execute_athena_query(sql)\n            \n            # Get AI interpretation\n            interpretation = self._interpret_results(question, df_result)\n            \n            return {\n                'sql': sql,\n                'data': df_result,\n                'insights': interpretation\n            }\n        except Exception as e:\n            return {\n                'error': str(e),\n                'sql': sql\n            }\n    \n    def _interpret_results(self, question, df_result):\n        \"\"\"Use Bedrock to interpret results\"\"\"\n        \n        results_summary = df_result.to_string() if len(df_result) < 20 else df_result.head(10).to_string()\n        \n        prompt = f\"\"\"As a {self.role}, analyze these results and provide insights.\n\nQuestion: {question}\n\nResults:\n{results_summary}\n\nProvide clear, actionable insights based on the data.\"\"\"\n        \n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        \n        response = bedrock_runtime.invoke_model(\n            modelId=MODEL_ID,\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"messages\": messages,\n                \"max_tokens\": 1000,\n                \"temperature\": 0.3\n            })\n        )\n        \n        response_body = json.loads(response['body'].read())\n        return response_body['content'][0]['text']\n\n# Create specialized agents\ncustomer_insights_agent = BedrockAgent(\n    name=\"Customer Insights Analyst\",\n    role=\"customer behavior analyst\",\n    system_prompt=\"Focus on user demographics, subscription patterns, and viewing behavior to provide customer insights.\"\n)\n\nmarketing_agent = BedrockAgent(\n    name=\"Marketing Performance Specialist\",\n    role=\"marketing analyst\",\n    system_prompt=\"Analyze campaign performance, ROI, and attribution to optimize marketing spend.\"\n)\n\nbi_agent = BedrockAgent(\n    name=\"Business Intelligence Analyst\",\n    role=\"senior business analyst\",\n    system_prompt=\"Provide comprehensive business insights combining user behavior, content performance, and marketing effectiveness.\"\n)\n\nprint(\"‚úÖ AI Agents created successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Specialized Agents\n",
    "\n",
    "We'll create different agents for different business functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze customer segments\nsegment_result = await customer_insights_agent.query(\n    \"\"\"Analyze our customer base:\n    1. What are the main customer segments by subscription type?\n    2. What's the average lifetime value for each segment?\n    3. Which segments show the highest engagement?\n    \"\"\"\n)\n\nif 'error' not in segment_result:\n    print(\"üìä Query Results:\")\n    display(segment_result['data'])\n    print(\"\\nüí° Insights:\")\n    print(segment_result['insights'])\nelse:\n    print(f\"‚ùå Error: {segment_result['error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer Insights Analysis\n",
    "\n",
    "Let's use the Customer Insights Agent to analyze user segments and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze customer segments\n",
    "segment_analysis = await customer_insights_agent.query(\n",
    "    \"\"\"Analyze our customer base:\n",
    "    1. What are the main customer segments by subscription type and demographics?\n",
    "    2. Which segments show the highest engagement with our streaming content?\n",
    "    3. Are there any underserved segments we should target?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(segment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Campaign performance analysis\ncampaign_result = await marketing_agent.query(\n    \"\"\"Analyze our ad campaigns:\n    1. Which campaigns have the best ROI?\n    2. What's the average cost per conversion by campaign type?\n    3. How do attribution models affect campaign performance?\n    \"\"\"\n)\n\nif 'error' not in campaign_result:\n    print(\"üìä Campaign Performance:\")\n    display(campaign_result['data'])\n    print(\"\\nüí° Marketing Insights:\")\n    print(campaign_result['insights'])\nelse:\n    print(f\"‚ùå Error: {campaign_result['error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Marketing Campaign Optimization\n",
    "\n",
    "Now let's use the Marketing Agent to optimize our ad campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campaign performance analysis\n",
    "campaign_analysis = await marketing_agent.query(\n",
    "    \"\"\"Analyze our current ad campaigns:\n",
    "    1. Which campaigns have the best ROI?\n",
    "    2. What are the main issues with underperforming campaigns?\n",
    "    3. How do attribution paths differ across platforms?\n",
    "    4. Provide specific optimization recommendations for our top 3 campaigns.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(campaign_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive business analysis\nexecutive_result = await bi_agent.query(\n    \"\"\"Create an executive summary:\n    1. Total active users by subscription tier\n    2. Average lifetime value trends\n    3. Top performing campaigns by ROI\n    4. Key growth opportunities\n    \"\"\"\n)\n\nif 'error' not in executive_result:\n    print(\"üìä Executive Dashboard:\")\n    display(executive_result['data'])\n    print(\"\\nüìà Executive Summary:\")\n    print(executive_result['insights'])\nelse:\n    print(f\"‚ùå Error: {executive_result['error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Executive Business Intelligence Report\n",
    "\n",
    "Let's use the BI Agent to create a comprehensive business report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive business analysis\n",
    "executive_report = await bi_agent.query(\n",
    "    \"\"\"Create an executive summary covering:\n",
    "    \n",
    "    1. Business Health Metrics:\n",
    "       - Total active users and growth trends\n",
    "       - Revenue by subscription tier\n",
    "       - Customer acquisition cost vs lifetime value\n",
    "    \n",
    "    2. Content Performance:\n",
    "       - Top performing content and genres\n",
    "       - User engagement metrics and trends\n",
    "       - Content ROI analysis\n",
    "    \n",
    "    3. Marketing Effectiveness:\n",
    "       - Overall marketing ROI\n",
    "       - Channel performance comparison\n",
    "       - Customer acquisition funnel analysis\n",
    "    \n",
    "    4. Strategic Recommendations:\n",
    "       - Top 3 growth opportunities\n",
    "       - Risk factors to monitor\n",
    "       - Resource allocation recommendations\n",
    "    \n",
    "    Present this as an executive-ready report with clear metrics and actionable insights.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(executive_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Analysis Examples\n",
    "\n",
    "Here are some interactive analysis examples you can try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Find similar users for targeted campaigns\n",
    "similar_users_analysis = await marketing_agent.query(\n",
    "    \"\"\"Find users similar to our top converters from campaign 'camp_001':\n",
    "    1. Identify the characteristics of users who converted\n",
    "    2. Find similar users who haven't been targeted yet\n",
    "    3. Estimate the potential ROI of targeting these users\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(similar_users_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Content recommendation strategy\n",
    "content_strategy = await customer_insights_agent.query(\n",
    "    \"\"\"Develop a content recommendation strategy:\n",
    "    1. Which users are most likely to upgrade to premium based on viewing patterns?\n",
    "    2. What content should we recommend to increase engagement?\n",
    "    3. How can we reduce churn for users with declining engagement?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(content_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Cross-sell opportunities\n",
    "cross_sell_analysis = await bi_agent.query(\n",
    "    \"\"\"Identify cross-sell opportunities:\n",
    "    1. Which free users show behavior patterns similar to premium subscribers?\n",
    "    2. What's the optimal timing for upgrade offers based on user journey?\n",
    "    3. Which marketing channels are most effective for upgrade campaigns?\n",
    "    4. Calculate potential revenue impact of a targeted upgrade campaign.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(cross_sell_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis Function\n",
    "\n",
    "Create a reusable function for common analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. MCP Server Integration Pattern\n\nThe AWS Data Processing MCP Server provides a standardized interface for AI agents.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cohort_analysis(agent, cohort_definition, metrics):\n",
    "    \"\"\"\n",
    "    Perform cohort analysis on user segments\n",
    "    \n",
    "    Args:\n",
    "        agent: The Strands agent to use\n",
    "        cohort_definition: How to define the cohort (e.g., \"users who joined in January\")\n",
    "        metrics: List of metrics to analyze (e.g., [\"retention\", \"engagement\", \"revenue\"])\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    Perform cohort analysis for {cohort_definition}:\n",
    "    \n",
    "    Analyze the following metrics:\n",
    "    {', '.join(metrics)}\n",
    "    \n",
    "    Provide:\n",
    "    1. Cohort size and characteristics\n",
    "    2. Metric trends over time\n",
    "    3. Comparison with other cohorts\n",
    "    4. Actionable insights for this cohort\n",
    "    \"\"\"\n",
    "    \n",
    "    return await agent.query(query)\n",
    "\n",
    "# Example usage\n",
    "premium_cohort_analysis = await cohort_analysis(\n",
    "    customer_insights_agent,\n",
    "    \"Premium subscribers who joined in the last 30 days\",\n",
    "    [\"content engagement\", \"viewing hours\", \"genre preferences\"]\n",
    ")\n",
    "\n",
    "print(premium_cohort_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Agent Collaboration\n",
    "\n",
    "Example of multiple agents working together on a complex analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary\n\nThis notebook demonstrated how to query the ACME Corp AWS data lakehouse using:\n\n1. **Amazon Bedrock Integration**: Used Claude 3.5 Sonnet for natural language to SQL conversion\n2. **Agent-Based Architecture**: Created specialized agents for different business functions\n3. **Real-Time Query Execution**: Connected to live data via Amazon Athena\n4. **AI-Powered Insights**: Generated business insights from query results\n5. **MCP Server Pattern**: Showed how to integrate with the AWS Data Processing MCP Server\n\n### Key Features Demonstrated:\n\n- **Natural Language Queries**: Ask questions in plain English\n- **Automatic SQL Generation**: AI converts questions to optimized SQL\n- **Real Data Access**: Query actual data stored in S3 via Athena\n- **Contextual Understanding**: Agents understand business context\n- **Actionable Insights**: Get specific recommendations, not just data\n\n### Performance Metrics:\n\n- Query execution: 400-1200ms (Athena)\n- SQL generation: ~1 second (Bedrock)\n- Total end-to-end: 2-4 seconds per query\n\n### Next Steps:\n\n1. **Production Deployment**: Set up proper authentication and monitoring\n2. **Query Optimization**: Add caching and query optimization\n3. **Enhanced Visualizations**: Add charts and dashboards\n4. **Real-Time Integration**: Connect to streaming data sources\n5. **Advanced Analytics**: Add predictive modeling capabilities\n\n### Resources:\n\n- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n- [AWS Data Processing MCP Server](https://github.com/awslabs/aws-dataprocessing-mcp-server)\n- [GitHub Repository](https://github.com/amitkalawat/data-agents-mcp-aws)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "Don't forget to clean up the MCP connections when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all MCP client connections\n",
    "async def cleanup():\n",
    "    await user_details_client.stop()\n",
    "    await streaming_analytics_client.stop()\n",
    "    await ad_campaign_client.stop()\n",
    "    print(\"All MCP clients stopped successfully!\")\n",
    "\n",
    "await cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# MCP Server Configuration Example\nmcp_config = {\n    \"mcpServers\": {\n        \"aws-dataprocessing\": {\n            \"command\": \"uvx\",\n            \"args\": [\n                \"awslabs.aws-dataprocessing-mcp-server@latest\",\n                \"--allow-write\"\n            ],\n            \"env\": {\n                \"AWS_REGION\": \"us-west-2\"\n            }\n        }\n    },\n    \"capabilities\": {\n        \"athena\": {\n            \"enabled\": True,\n            \"workgroup\": \"primary\",\n            \"database\": DATABASE_NAME,\n            \"output_location\": OUTPUT_LOCATION\n        },\n        \"glue\": {\n            \"enabled\": True,\n            \"catalog_id\": \"auto\"\n        }\n    }\n}\n\nprint(\"üìã MCP Server Configuration:\")\nprint(json.dumps(mcp_config, indent=2))\n\n# Example MCP tool usage patterns\nprint(\"\\nüõ†Ô∏è MCP Tools Available:\")\nprint(\"1. glue_data_catalog_handler - List tables, get schemas\")\nprint(\"2. athena_query_handler - Execute SQL queries\")\nprint(\"3. s3_handler - Read/write S3 objects\")\n\n# Simulate MCP tool call\ndef simulate_mcp_tool_call(tool_name, action, parameters):\n    \"\"\"Simulate what an MCP tool call would look like\"\"\"\n    \n    tool_request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"tools/call\",\n        \"params\": {\n            \"name\": tool_name,\n            \"arguments\": parameters\n        },\n        \"id\": 1\n    }\n    \n    print(f\"\\nüîß MCP Tool Call: {tool_name}\")\n    print(f\"Action: {action}\")\n    print(f\"Request:\")\n    print(json.dumps(tool_request, indent=2))\n    \n    # In real usage, this would be sent to the MCP server\n    # Here we'll just show what it would look like\n    \n    if tool_name == \"glue_data_catalog_handler\" and action == \"list_tables\":\n        return {\n            \"result\": {\n                \"tables\": list(schemas.keys()),\n                \"database\": DATABASE_NAME\n            }\n        }\n    \n    return {\"result\": \"Simulated response\"}\n\n# Example: List tables via MCP\ntables_response = simulate_mcp_tool_call(\n    \"glue_data_catalog_handler\",\n    \"list_tables\",\n    {\"database\": DATABASE_NAME}\n)\n\nprint(f\"\\nüìä Response:\")\nprint(json.dumps(tables_response, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Connect to MCP Servers**: Initialize connections to multiple data sources\n",
    "2. **Create Specialized Agents**: Build agents with specific roles and expertise\n",
    "3. **Perform Complex Analyses**: Use natural language to query and analyze data\n",
    "4. **Multi-Agent Collaboration**: Coordinate multiple agents for comprehensive insights\n",
    "5. **Generate Business Intelligence**: Create executive-ready reports and recommendations\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Natural Language Interface**: Query data using plain English\n",
    "- **Contextual Understanding**: Agents understand business context and objectives\n",
    "- **Cross-Functional Analysis**: Combine data from multiple sources seamlessly\n",
    "- **Actionable Insights**: Get specific recommendations, not just data\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Extend the MCP servers with more sophisticated analytics tools\n",
    "2. Create more specialized agents for specific business functions\n",
    "3. Build automated reporting workflows\n",
    "4. Integrate with real-time data streams for live monitoring\n",
    "5. Add visualization capabilities to the agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}